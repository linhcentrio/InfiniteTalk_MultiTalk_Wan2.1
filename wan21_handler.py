#!/usr/bin/env python3
"""
WAN2.1 InfiniteTalk/MultiTalk - Memory Optimized Handler v3.0
"""

import runpod
import os
import tempfile
import uuid
import requests
import time
import torch
import torch.nn.functional as F
import torchaudio
import sys
import gc
import json
import random
import traceback
import subprocess
import imageio
import numpy as np
import asyncio
import threading
from pathlib import Path
from minio import Minio
from urllib.parse import quote
import logging
from functools import wraps
from typing import Optional, Tuple, List, Dict, Any

# CRITICAL: Set memory environment variables tr∆∞·ªõc khi import torch operations
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512,garbage_collection_threshold:0.8'
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
os.environ['TORCH_BACKENDS_CUDNN_BENCHMARK'] = '1'

# Performance monitoring imports
try:
    import psutil
    import gpustat
    PERFORMANCE_MONITORING_AVAILABLE = True
except ImportError:
    PERFORMANCE_MONITORING_AVAILABLE = False
    print("‚ö†Ô∏è Performance monitoring packages not available")

# Configure enhanced logging v·ªõi memory tracking
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [MEM:%(process)d] - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/app/logs/wan21_memory.log') if os.path.exists('/app/logs') else logging.NullHandler()
    ]
)
logger = logging.getLogger(__name__)

# Add ComfyUI paths
sys.path.insert(0, '/app/ComfyUI')

# CRITICAL: Setup PyTorch memory management tr∆∞·ªõc khi import models
def setup_memory_management():
    """Setup aggressive memory management"""
    try:
        if torch.cuda.is_available():
            # Set memory fraction ƒë·ªÉ reserve memory cho processing
            torch.cuda.set_per_process_memory_fraction(0.95)  # Use max 95% GPU memory
            
            # Enable memory optimizations
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.set_float32_matmul_precision('medium')  # Balance performance vs precision
            
            # Clear any existing cache
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"üíæ GPU Memory Management Setup: {total_memory:.1f}GB total, 85% available for models")
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Memory management setup partially failed: {e}")

setup_memory_management()

# Enhanced attention mechanism detection v·ªõi memory considerations
def detect_available_attention_mechanisms():
    """Detect available attention mechanisms ∆∞u ti√™n memory efficiency"""
    mechanisms = {
        'flash_attn': False,
        'xformers': False, 
        'sageattention': False,
        'pytorch': True  # Always available fallback
    }
    
    # Test FlashAttention v·ªõi memory safety
    try:
        import flash_attn
        version = getattr(flash_attn, '__version__', 'unknown')
        if '2.7' <= version <= '2.8':
            mechanisms['flash_attn'] = True
            logger.info(f"‚úÖ FlashAttention detected - v{version} (memory efficient)")
        else:
            logger.warning(f"‚ö†Ô∏è FlashAttention v{version} - may have memory issues")
    except ImportError:
        logger.info("‚ö†Ô∏è FlashAttention not available")
    
    # Test XFormers
    try:
        import xformers
        mechanisms['xformers'] = True
        logger.info(f"‚úÖ XFormers detected - v{xformers.__version__} (good fallback)")
    except ImportError:
        logger.info("‚ö†Ô∏è XFormers not available")
    
    # Test SageAttention (th∆∞·ªùng memory hungry)
    try:
        import sageattention
        mechanisms['sageattention'] = True
        logger.info("‚úÖ SageAttention detected (may use more memory)")
    except ImportError:
        logger.info("‚ö†Ô∏è SageAttention not available")
    
    return mechanisms

ATTENTION_MECHANISMS = detect_available_attention_mechanisms()

# Import ComfyUI components v·ªõi memory management
try:
    from nodes import (
        CLIPLoader, CLIPTextEncode, LoadImage, CLIPVisionLoader, ImageScale
    )
    from custom_nodes.ComfyUI_WanVideoWrapper.nodes_model_loading import (
        WanVideoModelLoader, WanVideoVAELoader, WanVideoLoraSelect, WanVideoBlockSwap
    )
    from custom_nodes.ComfyUI_WanVideoWrapper.nodes import (
        WanVideoSampler, WanVideoTextEmbedBridge, WanVideoDecode, WanVideoClipVisionEncode,
        WanVideoContextOptions
    )
    from custom_nodes.ComfyUI_WanVideoWrapper.multitalk.nodes import (
        MultiTalkModelLoader, MultiTalkWav2VecEmbeds, WanVideoImageToVideoMultiTalk
    )
    from custom_nodes.ComfyUI_WanVideoWrapper.fantasytalking.nodes import DownloadAndLoadWav2VecModel
    from comfy_extras.nodes_audio import LoadAudio
    from custom_nodes.audio_separation_nodes_comfyui.src.separation import AudioSeparation
    from custom_nodes.audio_separation_nodes_comfyui.src.crop import AudioCrop
    
    logger.info("‚úÖ All WanVideoWrapper modules imported successfully")
    WANVIDEO_AVAILABLE = True
except ImportError as e:
    logger.error(f"‚ùå WanVideoWrapper import error: {e}")
    WANVIDEO_AVAILABLE = False

# MinIO Configuration
MINIO_ENDPOINT = "media.aiclip.ai"
MINIO_ACCESS_KEY = "VtZ6MUPfyTOH3qSiohA2"
MINIO_SECRET_KEY = "8boVPVIynLEKcgXirrcePxvjSk7gReIDD9pwto3t"
MINIO_BUCKET = "video"
MINIO_SECURE = False

try:
    minio_client = Minio(MINIO_ENDPOINT, access_key=MINIO_ACCESS_KEY, 
                        secret_key=MINIO_SECRET_KEY, secure=MINIO_SECURE)
    logger.info("‚úÖ MinIO client initialized")
except Exception as e:
    logger.error(f"‚ùå MinIO initialization failed: {e}")
    minio_client = None

# Model configurations
MODEL_CONFIGS = {
    "wan21_model": os.getenv("WAN21_MODEL_PATH", "/app/ComfyUI/models/diffusion_models/wan2.1-i2v-14b-480p-Q4_K_M.gguf"),
    "wan21_vae": os.getenv("WAN21_VAE_PATH", "/app/ComfyUI/models/vae/wan_2.1_vae.safetensors"),
    "text_encoder": os.getenv("UMT5_PATH", "/app/ComfyUI/models/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors"),
    "clip_vision": os.getenv("CLIP_VISION_PATH", "/app/ComfyUI/models/clip_vision/clip_vision_h.safetensors"),
    "multitalk_model": os.getenv("MULTITALK_MODEL_PATH", "/app/ComfyUI/models/diffusion_models/WanVideo_2_1_Multitalk_14B_fp8_e4m3fn.safetensors"),
    "infinitalk_model": os.getenv("INFINITALK_MODEL_PATH", "/app/ComfyUI/models/diffusion_models/Wan2_1-InfiniteTalk-Multi_fp16.safetensors"),
    "wav2vec_model": os.getenv("W2V_CKPT_PATH", "/app/ComfyUI/models/transformers/chinese-wav2vec2-base-fairseq-ckpt.pt"),
    "speed_lora": os.getenv("SPEED_LORA_PATH", "/app/ComfyUI/models/loras/lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors")
}

# Memory management configuration
MEMORY_CONFIG = {
    "max_gpu_usage_percent": 95,  # Max 95% GPU memory
    "emergency_cleanup_threshold": 98,  # Emergency cleanup at 98%
    "min_free_memory_gb": 6.0,  # Minimum free memory required
    "enable_aggressive_cleanup": True,
    "memory_monitoring": True
}

# CRITICAL: Emergency memory cleanup function
def emergency_memory_cleanup(force_aggressive=False):
    """Aggressive memory cleanup ƒë·ªÉ prevent OOM"""
    try:
        # Clear Python garbage
        collected = gc.collect()
        
        if torch.cuda.is_available():
            # Get current memory stats
            allocated_before = torch.cuda.memory_allocated() / 1024**3
            cached_before = torch.cuda.memory_reserved() / 1024**3
            
            # Clear CUDA cache
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            
            if force_aggressive:
                # More aggressive cleanup
                torch.cuda.synchronize()
                torch.cuda.reset_peak_memory_stats()
                # Clear all cached allocations
                torch.cuda.set_per_process_memory_fraction(0.9)  # Temporarily reduce
                torch.cuda.empty_cache()
                torch.cuda.set_per_process_memory_fraction(0.95)  # Restore
            
            allocated_after = torch.cuda.memory_allocated() / 1024**3
            cached_after = torch.cuda.memory_reserved() / 1024**3
            
            freed_memory = (allocated_before - allocated_after) + (cached_before - cached_after)
            
            logger.info(f"üßπ Memory cleanup: {allocated_after:.1f}GB allocated, {cached_after:.1f}GB cached")
            if freed_memory > 0.1:
                logger.info(f"‚úÖ Freed {freed_memory:.1f}GB memory")
        
        # Clear system variables if needed
        if force_aggressive:
            for name in list(globals().keys()):
                if name.startswith('_temp_') or name.startswith('cached_'):
                    try:
                        del globals()[name]
                    except:
                        pass
                        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Memory cleanup error: {e}")

def get_memory_stats():
    """Get current memory statistics"""
    if not torch.cuda.is_available():
        return {}
    
    try:
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        free = total - allocated
        usage_percent = (allocated / total) * 100
        
        return {
            "allocated_gb": allocated,
            "reserved_gb": reserved,
            "total_gb": total,
            "free_gb": free,
            "usage_percent": usage_percent
        }
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Failed to get memory stats: {e}")
        return {}

def memory_safe_execution(stage_name="Unknown"):
    """Decorator ƒë·ªÉ ensure memory safety cho functions"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Pre-execution memory check v√† cleanup
            mem_stats = get_memory_stats()
            if mem_stats.get("usage_percent", 0) > MEMORY_CONFIG["emergency_cleanup_threshold"]:
                logger.warning(f"üö® High memory usage before {stage_name}: {mem_stats.get('usage_percent', 0):.1f}%")
                emergency_memory_cleanup(force_aggressive=True)
            
            start_time = time.time()
            initial_memory = mem_stats.get("allocated_gb", 0)
            
            try:
                # Check if enough free memory
                if mem_stats.get("free_gb", 0) < MEMORY_CONFIG["min_free_memory_gb"]:
                    logger.warning(f"‚ö†Ô∏è Low free memory for {stage_name}: {mem_stats.get('free_gb', 0):.1f}GB")
                    emergency_memory_cleanup(force_aggressive=True)
                
                # Execute function
                result = func(*args, **kwargs)
                
                # Post-execution cleanup
                emergency_memory_cleanup()
                
                # Log performance
                duration = time.time() - start_time
                final_stats = get_memory_stats()
                memory_delta = final_stats.get("allocated_gb", 0) - initial_memory
                
                logger.info(f"‚úÖ {stage_name}: {duration:.1f}s, Memory: {final_stats.get('allocated_gb', 0):.1f}GB ({memory_delta:+.1f}GB)")
                
                return result
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    logger.error(f"üö® OOM Error in {stage_name}: {e}")
                    emergency_memory_cleanup(force_aggressive=True)
                    
                    # Try to recover by reducing parameters
                    if "kwargs" in locals() and isinstance(kwargs, dict):
                        # Force memory-saving parameters
                        kwargs["use_block_swap"] = True
                        kwargs["blocks_to_swap"] = min(30, kwargs.get("blocks_to_swap", 20) + 5)
                        kwargs["use_vae_tiling"] = True
                        logger.info(f"üîß Retrying {stage_name} v·ªõi memory-safe parameters")
                        
                        # Retry v·ªõi reduced parameters
                        try:
                            emergency_memory_cleanup(force_aggressive=True)
                            return func(*args, **kwargs)
                        except:
                            logger.error(f"‚ùå {stage_name} failed even v·ªõi memory-safe retry")
                            raise
                    else:
                        raise
                else:
                    raise
                    
        return wrapper
    return decorator

def get_optimal_attention_mode(prefer_memory_efficient=True) -> str:
    """Determine optimal attention mode ∆∞u ti√™n memory efficiency"""
    if prefer_memory_efficient:
        # ∆Øu ti√™n memory efficiency
        if ATTENTION_MECHANISMS['flash_attn']:
            return "flash_attn"  # Most memory efficient
        elif ATTENTION_MECHANISMS['xformers']:
            return "xformers"   # Good balance
        elif ATTENTION_MECHANISMS['sageattention']:
            return "sageattn"   # Fastest but more memory
        else:
            return "pytorch"    # Fallback
    else:
        # ∆Øu ti√™n performance
        if ATTENTION_MECHANISMS['sageattention']:
            return "sageattn"
        elif ATTENTION_MECHANISMS['flash_attn']:
            return "flash_attn"
        elif ATTENTION_MECHANISMS['xformers']:
            return "xformers"
        else:
            return "pytorch"

def auto_optimize_for_memory(job_input: dict) -> dict:
    """Auto-optimize parameters ƒë·ªÉ prevent memory issues"""
    logger.info("üîß Auto-optimizing parameters for memory efficiency...")
    
    # Get current memory state
    mem_stats = get_memory_stats()
    available_memory = mem_stats.get("free_gb", 30)  # Default assume 30GB if can't detect
    
    # Calculate complexity factors
    width = job_input.get("width", 400)
    height = job_input.get("height", 704)
    max_audio_duration = job_input.get("max_audio_duration", 30)
    
    resolution_factor = (width * height) / (400 * 704)
    duration_factor = max_audio_duration / 20  # 20s baseline
    
    complexity_score = resolution_factor * duration_factor
    
    # CRITICAL: Always enable block swap cho safety
    job_input["use_block_swap"] = True
    
    # Calculate optimal block swap based on complexity
    if complexity_score > 2.0 or available_memory < 15:
        job_input["blocks_to_swap"] = 30  # Aggressive swapping
        logger.info("üîß High complexity detected - enabling aggressive block swap (30 blocks)")
    elif complexity_score > 1.5 or available_memory < 20:
        job_input["blocks_to_swap"] = 25  # Moderate swapping
        logger.info("üîß Moderate complexity - enabling block swap (25 blocks)")
    else:
        job_input["blocks_to_swap"] = 20  # Standard swapping
        logger.info("üîß Standard complexity - enabling block swap (20 blocks)")
    
    # Auto-enable VAE tiling cho high resolution
    if width > 450 or height > 800 or available_memory < 12:
        job_input["use_vae_tiling"] = True
        logger.info("üîß Auto-enabled VAE tiling for memory efficiency")
    
    # Auto-reduce audio duration if needed
    if max_audio_duration > 25 and available_memory < 15:
        job_input["max_audio_duration"] = 20
        logger.info("üîß Auto-reduced audio duration to 20s for memory safety")
    elif max_audio_duration > 20 and available_memory < 10:
        job_input["max_audio_duration"] = 15
        logger.info("üîß Auto-reduced audio duration to 15s for critical memory safety")
    
    # Auto-reduce resolution if critical memory situation
    if available_memory < 10:
        new_width = min(width, 320)
        new_height = min(height, 576)
        if new_width != width or new_height != height:
            job_input["width"] = new_width
            job_input["height"] = new_height
            logger.warning(f"üîß Critical memory - auto-reduced resolution to {new_width}x{new_height}")
    
    # Force memory-efficient attention
    job_input["_force_memory_efficient_attention"] = available_memory < 15
    
    logger.info(f"üìä Optimization summary: {complexity_score:.1f} complexity, {available_memory:.1f}GB available")
    return job_input

@memory_safe_execution("Model Verification")
def verify_models(mode: str = "multitalk") -> Tuple[bool, List[str]]:
    """Verify required models v·ªõi memory safety"""
    logger.info(f"üîç Verifying models for {mode} mode...")
    missing_models = []
    existing_models = []
    
    base_models = ["wan21_model", "wan21_vae", "text_encoder", "clip_vision", "wav2vec_model"]
    
    if mode == "multitalk":
        required_models = base_models + ["multitalk_model"]
    else:
        required_models = base_models + ["infinitalk_model"]
    
    total_size = 0
    for name in required_models:
        path = MODEL_CONFIGS[name]
        if os.path.exists(path):
            try:
                file_size_mb = os.path.getsize(path) / (1024 * 1024)
                existing_models.append(f"{name}: {file_size_mb:.1f}MB")
                total_size += file_size_mb
            except Exception as e:
                missing_models.append(f"{name}: {path} (error reading)")
        else:
            missing_models.append(f"{name}: {path}")
    
    if missing_models:
        return False, missing_models
    else:
        logger.info(f"‚úÖ All {len(existing_models)} models verified! Total: {total_size:.1f}MB")
        return True, []

def ensure_odd_frames(frames: int) -> int:
    return frames if frames % 2 == 1 else frames + 1

def image_width_height(image_tensor):
    if image_tensor.ndim == 4:
        _, height, width, _ = image_tensor.shape
    elif image_tensor.ndim == 3:
        height, width, _ = image_tensor.shape
    else:
        raise ValueError(f"Unsupported image shape: {image_tensor.shape}")
    return width, height

@memory_safe_execution("Audio Processing")
def load_and_process_audio(audio_paths: List[str], max_duration: int = None):
    """Load v√† process audio v·ªõi memory optimization"""
    logger.info(f"üéµ Loading {len(audio_paths)} audio files v·ªõi memory optimization...")
    load_audio = LoadAudio()
    loaded_audios = []
    
    for i, path in enumerate(audio_paths):
        if path and os.path.exists(path):
            try:
                audio_data = load_audio.load(path)[0]
                loaded_audios.append(audio_data)
                logger.info(f"‚úÖ Audio {i+1} loaded: {audio_data['sample_rate']}Hz")
                
                # Memory cleanup after each audio load
                emergency_memory_cleanup()
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load audio {i+1}: {e}")
                loaded_audios.append(None)
        else:
            loaded_audios.append(None)
    
    # Process multiple audios
    non_none_audios = [a for a in loaded_audios if a is not None]
    if not non_none_audios:
        raise ValueError("No valid audio files loaded")
    
    if len(non_none_audios) > 1:
        logger.info("üîÑ Combining multiple audio files...")
        combined_waveform = torch.cat([a["waveform"] for a in non_none_audios], dim=-1)
        sample_rate = non_none_audios[0]["sample_rate"]
        combined_audio = {"waveform": combined_waveform, "sample_rate": sample_rate}
    else:
        combined_audio = non_none_audios[0]
    
    # Crop if max duration specified
    if max_duration:
        audio_duration = combined_audio["waveform"].shape[-1] / combined_audio["sample_rate"]
        if audio_duration > max_duration:
            logger.info(f"‚úÇÔ∏è Cropping audio from {audio_duration:.1f}s to {max_duration}s")
            audio_crop = AudioCrop()
            combined_audio = audio_crop.main(combined_audio, "00", f"{max_duration}")[0]
    
    return combined_audio, loaded_audios

@memory_safe_execution("Video Saving")
def save_video_with_audio(frames_tensor, output_path, fps, audio_path=None):
    """Save video v·ªõi memory-efficient processing"""
    try:
        logger.info(f"üé¨ Saving video to: {output_path}")
        
        # Convert tensor v·ªõi memory management
        if torch.is_tensor(frames_tensor):
            frames_np = frames_tensor.detach().cpu().float().numpy()
            del frames_tensor  # Free tensor memory immediately
            emergency_memory_cleanup()
        else:
            frames_np = np.array(frames_tensor, dtype=np.float32)
        
        # Handle batch dimension
        if frames_np.ndim == 5 and frames_np.shape[0] == 1:
            frames_np = frames_np[0]
        
        # Convert to uint8 v·ªõi proper scaling
        frames_np = np.clip(frames_np * 255.0, 0, 255).astype(np.uint8)
        logger.info(f"üìä Video stats: {frames_np.shape}, {frames_np.dtype}")
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        if audio_path and os.path.exists(audio_path):
            # Save with audio using optimized settings
            temp_video_path = output_path.replace('.mp4', '_temp.mp4')
            
            # Save video without audio v·ªõi memory-efficient writer
            with imageio.get_writer(temp_video_path, fps=fps, codec='h264', quality=6, macro_block_size=16) as writer:
                # Process frames in chunks ƒë·ªÉ avoid memory buildup
                chunk_size = min(50, len(frames_np) // 4)  # Process in chunks
                for i in range(0, len(frames_np), chunk_size):
                    chunk = frames_np[i:i+chunk_size]
                    for frame in chunk:
                        writer.append_data(frame)
                    # Cleanup after each chunk
                    del chunk
                    emergency_memory_cleanup()
            
            # Merge with audio
            cmd = [
                'ffmpeg', '-y', '-loglevel', 'error',
                '-i', temp_video_path, '-i', audio_path,
                '-c:v', 'libx264', '-preset', 'ultrafast',  # Fastest encoding
                '-c:a', 'aac', '-b:a', '96k',  # Compressed audio
                '-shortest', output_path
            ]
            
            try:
                subprocess.run(cmd, check=True, capture_output=True, timeout=180)
                os.remove(temp_video_path)
                logger.info("‚úÖ Video v·ªõi audio saved successfully")
            except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
                logger.warning(f"‚ö†Ô∏è FFmpeg failed: {e}")
                os.rename(temp_video_path, output_path)
        else:
            # Save without audio
            with imageio.get_writer(output_path, fps=fps, codec='h264', quality=6) as writer:
                chunk_size = min(50, len(frames_np) // 4)
                for i in range(0, len(frames_np), chunk_size):
                    chunk = frames_np[i:i+chunk_size]
                    for frame in chunk:
                        writer.append_data(frame)
                    del chunk
                    emergency_memory_cleanup()
        
        # Final cleanup
        del frames_np
        emergency_memory_cleanup()
        
        return output_path
        
    except Exception as e:
        logger.error(f"‚ùå Video saving failed: {e}")
        raise e

@memory_safe_execution("WAN2.1 Memory-Safe Generation")
def generate_video_wan21_memory_optimized(image_path: str, audio_paths: List[str], **kwargs) -> str:
    """
    MEMORY-OPTIMIZED WAN2.1 video generation
    Designed ƒë·ªÉ prevent CUDA OOM errors
    """
    try:
        logger.info("üé¨ Starting memory-optimized WAN2.1 generation...")
        
        # Auto-optimize parameters cho memory
        kwargs = auto_optimize_for_memory(kwargs)
        
        # Extract parameters
        positive_prompt = kwargs.get('positive_prompt', 'The person takes turns talking')
        negative_prompt = kwargs.get('negative_prompt', 'bright tones, overexposed, static, blurred details')
        
        width = kwargs.get('width', 400)
        height = kwargs.get('height', 704)
        fps = kwargs.get('fps', 25)
        seed = kwargs.get('seed', random.randint(1, 2**32 - 1))
        steps = kwargs.get('steps', 4)
        cfg_scale = kwargs.get('cfg_scale', 1.0)
        scheduler = kwargs.get('scheduler', 'flowmatch_distill')
        
        mode = kwargs.get('mode', 'multitalk')
        use_block_swap = kwargs.get('use_block_swap', True)  # Always True now
        blocks_to_swap = kwargs.get('blocks_to_swap', 20)
        use_speed_lora = kwargs.get('use_speed_lora', True)
        speed_lora_strength = kwargs.get('speed_lora_strength', 1.0)
        max_audio_duration = kwargs.get('max_audio_duration', None)
        use_vae_tiling = kwargs.get('use_vae_tiling', False)
        
        # Get memory-efficient attention mode
        prefer_memory_efficient = kwargs.get('_force_memory_efficient_attention', True)
        attention_mode = get_optimal_attention_mode(prefer_memory_efficient)
        
        logger.info(f"üéØ Memory-Optimized Parameters:")
        logger.info(f" Mode: {mode}, Attention: {attention_mode}")
        logger.info(f" Resolution: {width}x{height}, FPS: {fps}")
        logger.info(f" Block Swap: {use_block_swap} ({blocks_to_swap} blocks)")
        logger.info(f" VAE Tiling: {use_vae_tiling}")
        
        if not WANVIDEO_AVAILABLE:
            raise RuntimeError("WanVideoWrapper modules not available")
        
        with torch.inference_mode():
            # Initialize nodes
            logger.info("üîß Initializing nodes v·ªõi memory management...")
            
            # Core nodes
            clip_loader = CLIPLoader()
            clip_encode_positive = CLIPTextEncode()
            clip_encode_negative = CLIPTextEncode()
            clip_vision_loader = CLIPVisionLoader()
            load_image = LoadImage()
            image_scaler = ImageScale()
            
            # WanVideo nodes
            wan_text_embed_bridge = WanVideoTextEmbedBridge()
            wan_clip_vision = WanVideoClipVisionEncode()
            wan_vae_loader = WanVideoVAELoader()
            wan_vae_decoder = WanVideoDecode()
            multitalk_img2vid = WanVideoImageToVideoMultiTalk()
            
            # Audio nodes
            load_wav2vec = DownloadAndLoadWav2VecModel()
            multitalk_wav2vec = MultiTalkWav2VecEmbeds()
            
            # Model loading nodes
            wan_model_loader = WanVideoModelLoader()
            multitalk_loader = MultiTalkModelLoader()
            wan_lora_select = WanVideoLoraSelect()
            block_swapper = WanVideoBlockSwap()
            wan_sampler = WanVideoSampler()
            wan_context_options = WanVideoContextOptions()
            
            logger.info("‚úÖ All nodes initialized")
            
            # STAGE 1: Text encoding v·ªõi memory cleanup
            with memory_safe_execution("Text Encoding"):
                logger.info("üìù Loading Text Encoder...")
                clip = clip_loader.load_clip(os.path.basename(MODEL_CONFIGS["text_encoder"]), "wan", "default")[0]
                positive = clip_encode_positive.encode(clip, positive_prompt)[0]
                negative = clip_encode_negative.encode(clip, negative_prompt)[0]
                text_embeds = wan_text_embed_bridge.process(positive, negative)[0]
                del clip, positive, negative
                emergency_memory_cleanup()
            
            # STAGE 2: Image processing
            with memory_safe_execution("Image Processing"):
                logger.info("üñºÔ∏è Loading v√† processing image...")
                clip_vision = clip_vision_loader.load_clip(os.path.basename(MODEL_CONFIGS["clip_vision"]))[0]
                loaded_image = load_image.load_image(image_path)[0]
                
                orig_width, orig_height = image_width_height(loaded_image)
                logger.info(f"Original image: {orig_width}x{orig_height}")
                
                if orig_width != width or orig_height != height:
                    logger.info(f"Scaling image to {width}x{height}...")
                    loaded_image = image_scaler.upscale(loaded_image, "lanczos", width, height, "disabled")[0]
                
                # CLIP Vision processing v·ªõi memory efficiency
                clip_vision_output = wan_clip_vision.process(
                    clip_vision=clip_vision,
                    image_1=loaded_image,
                    strength_1=1.0,
                    strength_2=1.0,
                    force_offload=True,  # Enable offload
                    crop="disabled",
                    combine_embeds="average",
                    image_2=None,
                    negative_image=None,
                    tiles=0,
                    ratio=0.5
                )[0]
                del clip_vision
                emergency_memory_cleanup()
            
            # STAGE 3: Audio processing v·ªõi memory optimization
            with memory_safe_execution("Audio Processing"):
                combined_audio, loaded_audios = load_and_process_audio(audio_paths, max_audio_duration)
                
                audio_duration = combined_audio["waveform"].shape[-1] / combined_audio["sample_rate"]
                frames = max(1 * fps, int(audio_duration * fps))
                frames = ensure_odd_frames(frames)
                
                logger.info(f"Audio duration: {audio_duration:.1f}s, Frames: {frames}")
                
                # Save combined audio
                output_audio_path = "/tmp/combined_audio.wav"
                waveform = combined_audio["waveform"].squeeze(0)
                torchaudio.save(output_audio_path, waveform, combined_audio["sample_rate"])
                del waveform
                emergency_memory_cleanup()
            
            # STAGE 4: Wav2Vec processing
            with memory_safe_execution("Audio Embedding"):
                logger.info("üé§ Loading Wav2Vec model...")
                wav2vec_model = load_wav2vec.loadmodel("TencentGameMate/chinese-wav2vec2-base", "fp16", "main_device")[0]
                
                wav2vec_embeds, _, actual_num_frames = multitalk_wav2vec.process(
                    wav2vec_model, True, fps, frames,
                    loaded_audios[0] if loaded_audios else combined_audio,
                    1, 1, "para",
                    loaded_audios[1] if len(loaded_audios) > 1 else None,
                    loaded_audios[2] if len(loaded_audios) > 2 else None,
                    loaded_audios[3] if len(loaded_audios) > 3 else None
                )
                del wav2vec_model
                emergency_memory_cleanup()
            
            # STAGE 5: VAE v√† image embedding
            with memory_safe_execution("VAE & Image Embedding"):
                logger.info("üé® Loading VAE...")
                vae = wan_vae_loader.loadmodel(os.path.basename(MODEL_CONFIGS["wan21_vae"]), "fp16")[0]
                
                image_embeds = multitalk_img2vid.process(
                    vae, width, height, frames, fps,
                    False, 'mkl', loaded_image, False, clip_vision_output
                )[0]
                del loaded_image, clip_vision_output
                emergency_memory_cleanup()
            
            # STAGE 6: Talk model loading
            with memory_safe_execution("Talk Model Loading"):
                if mode == "multitalk":
                    logger.info("üó£Ô∏è Loading MultiTalk Model...")
                    talk_model_path = MODEL_CONFIGS["multitalk_model"]
                else:
                    logger.info("üó£Ô∏è Loading InfiniteTalk Model...")
                    talk_model_path = MODEL_CONFIGS["infinitalk_model"]
                
                multitalk_model = multitalk_loader.loadmodel(os.path.basename(talk_model_path), "fp16")[0]
            
            # STAGE 7: LoRA v√† Block Swap setup
            wan_speed_lora = None
            if use_speed_lora:
                logger.info("‚ö° Loading Speed LoRA...")
                wan_speed_lora = wan_lora_select.getlorapath(
                    os.path.basename(MODEL_CONFIGS["speed_lora"]),
                    speed_lora_strength, None, {}, None, False, False
                )[0]
            
            block_swap_args = None
            if use_block_swap:
                logger.info(f"üîÑ Setting up aggressive block swap ({blocks_to_swap} blocks)...")
                block_swap_args = block_swapper.setargs(
                    blocks_to_swap=blocks_to_swap,
                    offload_img_emb=True,   # Offload image embeddings
                    offload_txt_emb=True,   # Offload text embeddings
                    use_non_blocking=True,
                    vace_blocks_to_swap=0,
                    prefetch_blocks=1,      # Minimal prefetch
                    block_swap_debug=False
                )[0]
            
            # STAGE 8: Main model loading v·ªõi memory safety
            with memory_safe_execution("WAN2.1 Model Loading"):
                logger.info(f"üéØ Loading WAN2.1 base model v·ªõi {attention_mode} attention...")
                
                try:
                    model = wan_model_loader.loadmodel(
                        model=os.path.basename(MODEL_CONFIGS["wan21_model"]),
                        base_precision="fp16",
                        load_device="offload_device",
                        quantization="disabled",
                        compile_args=None,
                        attention_mode=attention_mode,
                        block_swap_args=block_swap_args,
                        lora=wan_speed_lora,
                        vram_management_args=None,
                        vace_model=None,
                        fantasytalking_model=None,
                        multitalk_model=multitalk_model,
                        fantasyportrait_model=None
                    )[0]
                    
                    logger.info(f"‚úÖ Model loaded successfully v·ªõi {attention_mode}")
                    
                except Exception as model_error:
                    logger.warning(f"‚ö†Ô∏è {attention_mode} failed: {model_error}")
                    logger.warning("üîÑ Retrying v·ªõi memory-safe pytorch attention...")
                    
                    # Emergency memory cleanup
                    emergency_memory_cleanup(force_aggressive=True)
                    
                    # Fallback to pytorch attention
                    model = wan_model_loader.loadmodel(
                        model=os.path.basename(MODEL_CONFIGS["wan21_model"]),
                        base_precision="fp16",
                        load_device="offload_device",
                        quantization="disabled",
                        compile_args=None,
                        attention_mode="pytorch",
                        block_swap_args=block_swap_args,
                        lora=wan_speed_lora,
                        vram_management_args=None,
                        vace_model=None,
                        fantasytalking_model=None,
                        multitalk_model=multitalk_model,
                        fantasyportrait_model=None
                    )[0]
                    
                    logger.info("‚úÖ Model loaded v·ªõi PyTorch attention fallback")
                
                del multitalk_model
                emergency_memory_cleanup()
            
            # STAGE 9: Context optimization cho long videos
            context_options = None
            if frames > 200:  # Use context optimization for videos > 8s
                context_frames = min(frames, 200)  # Smaller context for memory
                context_options = wan_context_options.process(
                    context_schedule="uniform_standard",
                    context_frames=context_frames,
                    context_stride=12,  # Larger stride
                    context_overlap=6,  # Smaller overlap
                    freenoise=True,
                    verbose=False,
                    image_cond_start_step=3,  # Earlier conditioning
                    image_cond_window_count=1,  # Fewer windows
                    vae=None,
                    fuse_method="linear",
                    reference_latent=None
                )[0]
                logger.info(f"üîß Memory-optimized context: {context_frames} frames, {context_options}")
            
            # STAGE 10: Video sampling v·ªõi memory management
            with memory_safe_execution("Video Sampling"):
                logger.info("üé¨ Sampling video v·ªõi memory optimization...")
                
                sampled = wan_sampler.process(
                    model=model,
                    image_embeds=image_embeds,
                    shift=8,
                    steps=steps,
                    cfg=cfg_scale,
                    seed=seed,
                    scheduler=scheduler,
                    riflex_freq_index=0,
                    text_embeds=text_embeds,
                    force_offload=True,  # Force offload for memory
                    samples=None,
                    feta_args=None,
                    denoise_strength=1.0,
                    context_options=context_options,
                    cache_args=None,
                    teacache_args=None,
                    flowedit_args=None,
                    batched_cfg=False,
                    slg_args=None,
                    rope_function="default",
                    loop_args=None,
                    experimental_args=None,
                    sigmas=None,
                    unianimate_poses=None,
                    fantasytalking_embeds=None,
                    uni3c_embeds=None,
                    multitalk_embeds=wav2vec_embeds,
                    freeinit_args=None,
                    start_step=0,
                    end_step=-1,
                    add_noise_to_samples=False
                )[0]
                
                del model, text_embeds, wav2vec_embeds
                emergency_memory_cleanup()
            
            # STAGE 11: VAE decoding v·ªõi tiling
            with memory_safe_execution("VAE Decoding"):
                logger.info("üé® Decoding latents v·ªõi memory-efficient VAE tiling...")
                
                decoded = wan_vae_decoder.decode(
                    vae=vae,
                    samples=sampled,
                    enable_vae_tiling=use_vae_tiling,
                    tile_x=256 if use_vae_tiling else 512,  # Smaller tiles
                    tile_y=256 if use_vae_tiling else 512,
                    tile_stride_x=128 if use_vae_tiling else 256,
                    tile_stride_y=128 if use_vae_tiling else 256,
                    normalization="default"
                )[0]
                
                del vae, sampled, image_embeds
                emergency_memory_cleanup()
            
            # STAGE 12: Save video
            output_path = f"/app/ComfyUI/output/wan21_{mode}_{uuid.uuid4().hex[:8]}.mp4"
            final_output_path = save_video_with_audio(decoded, output_path, fps, output_audio_path)
            
            # Final cleanup
            del decoded
            emergency_memory_cleanup()
            
            logger.info(f"‚úÖ Memory-optimized video generation completed: {final_output_path}")
            return final_output_path
            
    except Exception as e:
        logger.error(f"‚ùå Memory-optimized generation failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        emergency_memory_cleanup(force_aggressive=True)
        return None
    finally:
        emergency_memory_cleanup(force_aggressive=True)

def upload_to_minio(local_path: str, object_name: str) -> str:
    """Upload v·ªõi retry logic v√† memory safety"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            if not minio_client:
                raise RuntimeError("MinIO client not initialized")
            
            if not os.path.exists(local_path):
                raise FileNotFoundError(f"Local file not found: {local_path}")
            
            file_size_mb = os.path.getsize(local_path) / (1024 * 1024)
            logger.info(f"üì§ Uploading: {object_name} ({file_size_mb:.1f}MB) - Attempt {attempt + 1}")
            
            minio_client.fput_object(MINIO_BUCKET, object_name, local_path)
            
            file_url = f"https://{MINIO_ENDPOINT}/{MINIO_BUCKET}/{quote(object_name)}"
            logger.info(f"‚úÖ Upload completed: {file_url}")
            return file_url
            
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                logger.warning(f"‚ö†Ô∏è Upload attempt {attempt + 1} failed: {e}, retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                logger.error(f"‚ùå Upload failed after {max_retries} attempts: {e}")
                raise e

def validate_input_parameters(job_input: dict) -> Tuple[bool, str]:
    """Enhanced input validation"""
    try:
        required_params = ["image_url", "audio_urls"]
        for param in required_params:
            if param not in job_input or not job_input[param]:
                return False, f"Missing required parameter: {param}"
        
        # Validate image URL
        try:
            response = requests.head(job_input["image_url"], timeout=15)
            if response.status_code != 200:
                return False, f"Image URL not accessible: {response.status_code}"
        except Exception as e:
            return False, f"Image URL validation failed: {str(e)}"
        
        # Validate audio URLs
        audio_urls = job_input["audio_urls"]
        if not isinstance(audio_urls, list) or len(audio_urls) == 0:
            return False, "audio_urls must be a non-empty list"
        
        if len(audio_urls) > 4:
            return False, "Maximum 4 audio files supported"
        
        # Validate dimensions
        width = job_input.get("width", 400)
        height = job_input.get("height", 704)
        if not (256 <= width <= 800 and 256 <= height <= 800):  # Reasonable limits cho memory
            return False, "Width and height must be between 256 and 800 for memory safety"
        
        # Validate other parameters
        mode = job_input.get("mode", "multitalk")
        if mode not in ["multitalk", "infinitetalk"]:
            return False, "Mode must be 'multitalk' or 'infinitetalk'"
        
        steps = job_input.get("steps", 4)
        if not (1 <= steps <= 15):  # Reduced max steps
            return False, "Steps must be between 1 and 15"
        
        fps = job_input.get("fps", 25)
        if not (15 <= fps <= 30):
            return False, "FPS must be between 15 and 30"
        
        return True, "Valid"
        
    except Exception as e:
        return False, f"Parameter validation error: {str(e)}"

@memory_safe_execution("Complete Job Processing")
def handler(job):
    """Enhanced RunPod handler v·ªõi comprehensive memory management"""
    job_id = job.get("id", "unknown")
    start_time = time.time()
    
    try:
        job_input = job.get("input", {})
        
        # Log initial memory state
        initial_mem = get_memory_stats()
        logger.info(f"üöÄ Job {job_id} started - Initial memory: {initial_mem.get('allocated_gb', 0):.1f}GB")
        
        # Validate input
        is_valid, validation_message = validate_input_parameters(job_input)
        if not is_valid:
            return {"error": validation_message, "status": "failed", "job_id": job_id}
        
        # Extract parameters
        image_url = job_input["image_url"]
        audio_urls = job_input["audio_urls"]
        
        parameters = {
            "positive_prompt": job_input.get("positive_prompt", "The person takes turns talking"),
            "negative_prompt": job_input.get("negative_prompt", "bright tones, overexposed, static, blurred details"),
            "width": job_input.get("width", 400),
            "height": job_input.get("height", 704),
            "fps": job_input.get("fps", 25),
            "seed": job_input.get("seed", 0),
            "steps": job_input.get("steps", 4),
            "cfg_scale": job_input.get("cfg_scale", 1.0),
            "scheduler": job_input.get("scheduler", "flowmatch_distill"),
            "mode": job_input.get("mode", "multitalk"),
            "use_block_swap": job_input.get("use_block_swap", True),  # Default True
            "blocks_to_swap": job_input.get("blocks_to_swap", 20),
            "use_speed_lora": job_input.get("use_speed_lora", True),
            "speed_lora_strength": job_input.get("speed_lora_strength", 1.0),
            "max_audio_duration": job_input.get("max_audio_duration", None),
            "use_vae_tiling": job_input.get("use_vae_tiling", False)
        }
        
        logger.info(f"üöÄ Job {job_id}: WAN2.1 {parameters['mode'].upper()} Generation Started")
        logger.info(f"üñºÔ∏è Image: {image_url}")
        logger.info(f"üéµ Audio files: {len(audio_urls)}")
        logger.info(f"‚öôÔ∏è Resolution: {parameters['width']}x{parameters['height']} @ {parameters['fps']}fps")
        
        # Verify models
        models_ok, missing_models = verify_models(parameters['mode'])
        if not models_ok:
            return {
                "error": "Required models are missing",
                "missing_models": missing_models,
                "status": "failed",
                "mode": parameters['mode']
            }
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # Download files v·ªõi memory monitoring
            with memory_safe_execution("File Downloads"):
                # Download image
                image_path = os.path.join(temp_dir, "input_image.jpg")
                logger.info("üì• Downloading input image...")
                
                response = requests.get(image_url, timeout=60, stream=True)
                response.raise_for_status()
                
                with open(image_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                # Download audio files
                audio_paths = []
                for i, audio_url in enumerate(audio_urls):
                    if audio_url:
                        audio_path = os.path.join(temp_dir, f"audio_{i}.wav")
                        try:
                            response = requests.get(audio_url, timeout=60, stream=True)
                            response.raise_for_status()
                            
                            with open(audio_path, 'wb') as f:
                                for chunk in response.iter_content(chunk_size=8192):
                                    if chunk:
                                        f.write(chunk)
                            
                            audio_paths.append(audio_path)
                            logger.info(f"‚úÖ Audio {i+1} downloaded")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Failed to download audio {i+1}: {e}")
                            audio_paths.append(None)
                    else:
                        audio_paths.append(None)
                
                valid_audio_paths = [p for p in audio_paths if p is not None]
                if not valid_audio_paths:
                    return {"error": "No valid audio files could be downloaded"}
            
            # Generate video v·ªõi memory optimization
            generation_start = time.time()
            output_path = generate_video_wan21_memory_optimized(
                image_path=image_path,
                audio_paths=audio_paths,
                **parameters
            )
            generation_time = time.time() - generation_start
            
            if not output_path or not os.path.exists(output_path):
                return {"error": "Video generation failed"}
            
            # Upload result
            logger.info("üì§ Uploading result...")
            output_filename = f"wan21_memory_optimized_{parameters['mode']}_{job_id}_{uuid.uuid4().hex[:8]}.mp4"
            
            try:
                output_url = upload_to_minio(output_path, output_filename)
            except Exception as e:
                return {"error": f"Failed to upload result: {str(e)}"}
            
            # Calculate statistics
            total_time = time.time() - start_time
            file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
            final_mem = get_memory_stats()
            
            logger.info(f"‚úÖ Job {job_id} completed successfully!")
            logger.info(f"‚è±Ô∏è Total: {total_time:.1f}s (generation: {generation_time:.1f}s)")
            logger.info(f"üìä Output: {file_size_mb:.1f}MB")
            logger.info(f"üíæ Peak memory: {final_mem.get('allocated_gb', 0):.1f}GB")
            
            return {
                "output_video_url": output_url,
                "processing_time_seconds": round(total_time, 2),
                "generation_time_seconds": round(generation_time, 2),
                "video_info": {
                    "width": parameters["width"],
                    "height": parameters["height"],
                    "fps": parameters["fps"],
                    "file_size_mb": round(file_size_mb, 2),
                    "mode": parameters["mode"],
                    "audio_files_used": len(valid_audio_paths)
                },
                "generation_params": {
                    "positive_prompt": parameters["positive_prompt"],
                    "steps": parameters["steps"],
                    "cfg_scale": parameters["cfg_scale"],
                    "seed": parameters["seed"] if parameters["seed"] != 0 else "auto-generated",
                    "scheduler": parameters["scheduler"],
                    "mode": parameters["mode"],
                    "attention_mode": get_optimal_attention_mode(True),
                    "block_swap": parameters["use_block_swap"],
                    "blocks_swapped": parameters["blocks_to_swap"],
                    "vae_tiling": parameters["use_vae_tiling"],
                    "workflow_version": "WAN21_MEMORY_OPTIMIZED_v3.0"
                },
                "memory_info": {
                    "initial_memory_gb": round(initial_mem.get('allocated_gb', 0), 1),
                    "peak_memory_gb": round(final_mem.get('allocated_gb', 0), 1),
                    "memory_efficiency": "optimized_for_40GB_GPU"
                },
                "status": "completed"
            }
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"‚ùå Handler error for job {job_id}: {error_msg}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # Emergency cleanup
        emergency_memory_cleanup(force_aggressive=True)
        
        return {
            "error": error_msg,
            "status": "failed",
            "processing_time_seconds": round(time.time() - start_time, 2),
            "job_id": job_id,
            "debug_info": {
                "attention_mechanisms": ATTENTION_MECHANISMS,
                "memory_config": MEMORY_CONFIG,
                "memory_stats": get_memory_stats()
            }
        }
    finally:
        emergency_memory_cleanup(force_aggressive=True)

def health_check():
    """Enhanced health check v·ªõi memory monitoring"""
    try:
        # Check CUDA
        if not torch.cuda.is_available():
            return False, "CUDA not available"
        
        # Check memory situation
        mem_stats = get_memory_stats()
        available_memory = mem_stats.get("free_gb", 0)
        
        if available_memory < 5:
            return False, f"Insufficient GPU memory: {available_memory:.1f}GB available"
        
        # Check modules
        if not WANVIDEO_AVAILABLE:
            return False, "WanVideoWrapper not available"
        
        # Check models
        multitalk_ok, _ = verify_models("multitalk")
        infinitetalk_ok, _ = verify_models("infinitetalk")
        
        if not (multitalk_ok or infinitetalk_ok):
            return False, "No complete model sets available"
        
        # Check MinIO
        if not minio_client:
            return False, "MinIO not available"
        
        # Available features
        available_modes = []
        if multitalk_ok:
            available_modes.append("multitalk")
        if infinitetalk_ok:
            available_modes.append("infinitetalk")
        
        attention_modes = [name for name, available in ATTENTION_MECHANISMS.items() if available]
        optimal_attention = get_optimal_attention_mode(True)
        
        total_memory = mem_stats.get("total_gb", 0)
        
        health_info = (
            f"Ready - Modes: {', '.join(available_modes)}, "
            f"Memory: {available_memory:.1f}GB/{total_memory:.1f}GB available, "
            f"Attention: {optimal_attention} (available: {', '.join(attention_modes)}), "
            f"Memory Optimized: v3.0"
        )
        
        return True, health_info
        
    except Exception as e:
        return False, f"Health check failed: {str(e)}"

if __name__ == "__main__":
    logger.info("üöÄ Starting WAN2.1 Memory-Optimized Serverless Worker v3.0...")
    logger.info(f"üî• PyTorch: {torch.__version__}")
    logger.info(f"üéØ CUDA Available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        gpu_props = torch.cuda.get_device_properties(0)
        logger.info(f"üíæ GPU: {gpu_props.name}")
        logger.info(f"üíæ GPU Memory: {gpu_props.total_memory / 1e9:.1f}GB")
        
        # Log memory management settings
        mem_stats = get_memory_stats()
        logger.info(f"üìä Memory Management: {MEMORY_CONFIG['max_gpu_usage_percent']}% limit, {mem_stats.get('free_gb', 0):.1f}GB available")
    
    # Log optimizations
    attention_modes = [name for name, available in ATTENTION_MECHANISMS.items() if available]
    logger.info(f"‚ö° Available Attention: {attention_modes}")
    logger.info(f"üßπ Memory Optimizations: Enhanced cleanup, Block swap default, VAE tiling support")
    
    try:
        # Health check
        health_ok, health_msg = health_check()
        if not health_ok:
            logger.error(f"‚ùå Health check failed: {health_msg}")
            sys.exit(1)
        
        logger.info(f"‚úÖ Health check passed: {health_msg}")
        logger.info("üé¨ Ready to process memory-optimized WAN2.1 requests...")
        logger.info("üîß Memory management: Aggressive cleanup, CUDA OOM prevention, Auto-optimization!")
        
        # Start RunPod worker
        runpod.serverless.start({"handler": handler})
        
    except Exception as e:
        logger.error(f"‚ùå Startup failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)
